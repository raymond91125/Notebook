{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raymond91125/Notebook/blob/master/HostLlama2BehindAPI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hosting Llama 2 with Free GPU via Google Collab**"
      ],
      "metadata": {
        "id": "bznyRLfLg750"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://medium.com/@yuhongsun96/host-a-llama-2-api-on-gpu-for-free-a5311463c183"
      ],
      "metadata": {
        "id": "1pXF584qIGuo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Before getting started, if running on Google Colab, check that the runtime is set to T4 GPU**"
      ],
      "metadata": {
        "id": "HRDd_-CFIqRV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Dependencies\n",
        "- Requirements for running FastAPI Server\n",
        "- Requirements for creating a public model serving URL via Ngrok\n",
        "- Requirements for running Llama2 13B (including Quantization)\n"
      ],
      "metadata": {
        "id": "kU0GYiR4hWKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build Llama cpp\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python"
      ],
      "metadata": {
        "id": "gDVaaatLEpzq",
        "outputId": "da9d05c4-3f74-4010-e4a4-17abedf23e24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.11.tar.gz (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.23.5)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (5.6.3)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.11-cp310-cp310-manylinux_2_35_x86_64.whl size=6423607 sha256=1a10c4c3de05174b2408bd85160f8778f5ad3a700d7fb1c38b14a6c20897d3f9\n",
            "  Stored in directory: /root/.cache/pip/wheels/dc/42/77/a3ab0d02700427ea364de5797786c0272779dce795f62c3bc2\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: llama-cpp-python\n",
            "Successfully installed llama-cpp-python-0.2.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u24fGvOmT2Pi"
      },
      "outputs": [],
      "source": [
        "# If this complains about dependency resolver, it's safe to ignore\n",
        "!pip install fastapi[all] uvicorn python-multipart transformers pydantic tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This downloads and sets up the Ngrok executable in the Google Colab instance\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip -o ngrok-stable-linux-amd64.zip"
      ],
      "metadata": {
        "id": "Kxx3xpr2cXHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ngrok is used to make the FastAPI server accessible via a public URL.\n",
        "\n",
        "Users are required to make a free account and provide their auth token to use Ngrok. The free version only allows 1 local tunnel and the auth token is used to track this usage limit."
      ],
      "metadata": {
        "id": "WPOYhvtAHiW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://dashboard.ngrok.com/signup\n",
        "!./ngrok authtoken <YOUR-NGROK-TOKEN-HERE>"
      ],
      "metadata": {
        "id": "Nw3LWCfNcg9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create FastAPI App\n",
        "This provides an API to the Llama 2 model. The model version can be changed in the code below as desired.\n",
        "\n",
        "For this demo we will use the 13 billion parameter version which is finetuned for instruction (chat) following.\n",
        "\n",
        "Despite the compression, it is still a more powerful model than the 7B variant."
      ],
      "metadata": {
        "id": "vUqG0_-6IChq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "from typing import Any\n",
        "\n",
        "from fastapi import FastAPI\n",
        "from fastapi import HTTPException\n",
        "from pydantic import BaseModel\n",
        "from huggingface_hub import hf_hub_download\n",
        "from llama_cpp import Llama\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# GGML model required to fit Llama2-13B on a T4 GPU\n",
        "GENERATIVE_AI_MODEL_REPO = \"TheBloke/Llama-2-13B-chat-GGML\"\n",
        "GENERATIVE_AI_MODEL_FILE = \"llama-2-13b-chat.ggmlv3.q5_1.bin\"\n",
        "\n",
        "model_path = hf_hub_download(\n",
        "    repo_id=GENERATIVE_AI_MODEL_REPO,\n",
        "    filename=GENERATIVE_AI_MODEL_FILE\n",
        ")\n",
        "\n",
        "llama2_model = Llama(\n",
        "    model_path=model_path,\n",
        "    n_gpu_layers=64,\n",
        "    n_ctx=2000\n",
        ")\n",
        "\n",
        "# Test an inference\n",
        "print(llama2_model(prompt=\"Hello \", max_tokens=1))\n",
        "\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "\n",
        "# This defines the data json format expected for the endpoint, change as needed\n",
        "class TextInput(BaseModel):\n",
        "    inputs: str\n",
        "    parameters: dict[str, Any] | None\n",
        "\n",
        "\n",
        "@app.get(\"/\")\n",
        "def status_gpu_check() -> dict[str, str]:\n",
        "    gpu_msg = \"Available\" if tf.test.is_gpu_available() else \"Unavailable\"\n",
        "    return {\n",
        "        \"status\": \"I am ALIVE!\",\n",
        "        \"gpu\": gpu_msg\n",
        "    }\n",
        "\n",
        "\n",
        "@app.post(\"/generate/\")\n",
        "async def generate_text(data: TextInput) -> dict[str, str]:\n",
        "    try:\n",
        "        params = data.parameters or {}\n",
        "        response = llama2_model(prompt=data.inputs, **params)\n",
        "        model_out = response['choices'][0]['text']\n",
        "        return {\"generated_text\": model_out}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))"
      ],
      "metadata": {
        "id": "Z6F078OHVssf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start FastAPI Server\n",
        "The initial run will take a long time due to having to download the model and load it onto GPU.\n",
        "\n",
        "Note: interrupting the Google Colab runtime will send a SIGINT and stop the server."
      ],
      "metadata": {
        "id": "PikyQUQKIewj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell finishes quickly because it just needs to start up the server\n",
        "# The server will start the model download and will take a while to start up\n",
        "# ~5 minutes\n",
        "!uvicorn app:app --host 0.0.0.0 --port 8000 > server.log 2>&1 &"
      ],
      "metadata": {
        "id": "HErEYVtPcGg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the logs at server.log to see progress.\n",
        "\n",
        "Wait until model is loaded and check with the next cell before moving on."
      ],
      "metadata": {
        "id": "tunHZmTHX98z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If you see \"Failed to connect\", it's because the server is still starting up\n",
        "# Wait for the model to be downloaded and the server to fully start\n",
        "# Check the server.log file to see the status\n",
        "!curl localhost:8000"
      ],
      "metadata": {
        "id": "pJUJ4vICfQ7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use Ngrok to create a public URL for the FastAPI server.\n",
        "**IMPORTANT:** If you created an account via email, please verify your email or the next 2 cells won't work.\n",
        "\n",
        "If you signed up via Google or GitHub account, you're good to go."
      ],
      "metadata": {
        "id": "csO2FA7LKWXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This starts Ngrok and creates the public URL\n",
        "from IPython import get_ipython\n",
        "get_ipython().system_raw('./ngrok http 8000 &')"
      ],
      "metadata": {
        "id": "c-FpVEkifNdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the URL generated by the next cell, it should report that the FastAPI server is alive and that GPU is available.\n",
        "\n",
        "To hit the model endpoint, simply add `/generate` to the URL"
      ],
      "metadata": {
        "id": "XxzFtg3_Kfot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the Public URL\n",
        "# If this doesn't work, make sure you verified your email\n",
        "# Then run the previous code cell and this one again\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "metadata": {
        "id": "aCAIkVuxc3JU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shutting Down\n",
        "To shut down the processes, run the following commands in a new cell:\n",
        "```\n",
        "!pkill uvicorn\n",
        "!pkill ngrok\n",
        "```"
      ],
      "metadata": {
        "id": "liqVEsGfZPse"
      }
    }
  ]
}